{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71e9bc37",
   "metadata": {},
   "source": [
    "# COM4509/6509 MLAI - Assignment Part 2 Brief\n",
    "\n",
    "## Deadline: Friday, December 3, 2021 at 15:00 hrs\n",
    "\n",
    "## Scope: Sessions 6 to 8\n",
    "\n",
    "## Number of marks available for Part 2: 25\n",
    "\n",
    "### Please READ the whole assignment first, before starting to work on it.\n",
    "\n",
    "### How and what to submit\n",
    "\n",
    "1. A Jupyter Notebook with the code in all the cells executed, outputs displayed, and code\n",
    "documented.\n",
    "\n",
    "2. Name your Notebook as COM4509-6509_Assignment_Part2_Username_XXXXXX.ipynb where XXXXXX is your username such as abc18de.\n",
    "\n",
    "3. Upload a .zip file to Blackboard before the deadline that contains two Jupyter Notebooks, one for Part 1 and one for Part 2 (COM4509-6509_Assignment_Part1_Username_XXXXXX.ipynb and COM4509-6509_Assignment_Part2_Username_XXXXXX.ipynb)\n",
    "\n",
    "4. NO DATA UPLOAD: Please do not upload the data files used in this Notebook. We have a copy already. Instead, please use a relative file path in your code (data files under folder ‘data’), as in the lab notebook so that we can run your code smoothly when needed. So ‘./data/’, instead of ‘/User/username/myfiles/mlai/assignment1/’\n",
    "\n",
    "### Assessment Criteria\n",
    "\n",
    "1) Being able to build complete, reproducible machine learning pipelines from loading data to evaluating prediction performance.\n",
    "\n",
    "2) Being able to design different machine learning models to compare/optimise prediction performance.\n",
    "\n",
    "3) Being able to perform exploratory data analysis to gain insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cdbd8c",
   "metadata": {},
   "source": [
    "## A. Reproducibility & readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01705b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(210169508) # Set a random seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b7239a",
   "metadata": {},
   "source": [
    "## B. Logistic Regression on BreastMNIST [9 marks]\n",
    "\n",
    "The first version of the MedMNIST, published in ISBI21 (2010.14925.pdf (arxiv.org)). As taken from the paper: BreastMNIST is based on a dataset of 780 breast ultrasound images. It is categorized into 3 classes: normal, benign and malignant originally but in BreastMNIST, the task is simplified into binary classification by combining normal and benign as positive, and classifying them against malignant as negative. The source dataset with a ratio of 7 : 1 : 2 into training, validation and test set. The source images of 1 × 500 × 500 are resized into 1 × 28 × 28.\n",
    "\n",
    "We aim to train a L2-regularised logistic regression model to classify the two classes in BreastMNIST using the standard train/validation/test split with decent performance, i.e. much better than the chance level at worst."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172bd8c0",
   "metadata": {},
   "source": [
    "### B1 Data loading and inspection [3 mark]\n",
    "\n",
    "Follow instructions at https://github.com/MedMNIST/MedMNIST to download and load the data. Display at least ten images for each class, i.e. at least 20 images, from the training set. Display at least ten images for each class from the validation set, and display at least ten images for each class from the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b4e0ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98ff2d10",
   "metadata": {},
   "source": [
    "### B2 Logistic regression [4 marks]\n",
    "\n",
    "Keep a record of the three metrics M1 to M3 below for the two models below:\n",
    "* M1) Training accuracy: the prediction accuracy of a trained model on the training dataset.\n",
    "* M2) Validation accuracy: the prediction accuracy of a trained model on the validation dataset.\n",
    "* M3) Testing accuracy: the prediction accuracy of a trained model on the test dataset.\n",
    "\n",
    "**a.** Using the built-in logistic regression functions in scikit-learn, train a logistic regression model with L2 regularisation on the training set, use the validation set to choose a good regularisation parameter (a hyperparameter) from at least three choices, and test the chosen model on the test set. Report the three metrics M1 to M3 **[2 marks]**\n",
    "\n",
    "**b.** Using PyTorch (see Question 5 of Lab 6), train a logistic regression model with L2 regularisation on the training set, use the validation set to choose a good regularisation parameter (a hyperparameter) from at least three choices, and test the chosen model on the test set. Report the three metrics M1 to M3 **[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceadc057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f04e0b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15029bf8",
   "metadata": {},
   "source": [
    "### B3 Performance comparison (2 marks)\n",
    "\n",
    "**a.** Summarise each of the three metrics from the two models in B2 using one or more bar graphs. **[1 mark]**\n",
    "\n",
    "**b.** Describe at least two observations interesting to you. **[1 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5471e88c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd98081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be28bb11",
   "metadata": {},
   "source": [
    "## C. Convolutional Neural Networks on OCTMNIST [8 marks]\n",
    "\n",
    "OCTMNIST is based on a prior dataset of 109,309 valid optical coherence tomography (OCT) images for retinal diseases, with 4 different types, leading to a multi-class classification task. The source training set is split with a ratio of 9 : 1 into training and validation sets, and uses its source validation set as the test set. The source images are single channel, and their sizes are (384−1, 536)×(277−512), which are center-cropped and resized to 1 × 28 × 28."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2945a27",
   "metadata": {},
   "source": [
    "### C1 Data loading and inspection [2 mark]\n",
    "\n",
    "Follow instructions at https://github.com/MedMNIST/MedMNIST to download and load the data. Display at least ten images for each class, i.e. at least 40 images, from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5b565c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1542769a",
   "metadata": {},
   "source": [
    "### C2 Convolutional neural networks [4 marks]\n",
    "\n",
    "Keep a record of the four metrics M1 to M4 below for the two models below:\n",
    "* M1) Training accuracy: the prediction accuracy of a trained model on the training dataset. \n",
    "* M2) Validation accuracy: the prediction accuracy of a trained model on the validation dataset. \n",
    "* M3) Testing accuracy: the prediction accuracy of a trained model on the test dataset.\n",
    "* M4) Training time: the time taken to train the model (i.e. to learn/estimate the learnable parameters) on the training dataset.\n",
    "\n",
    "This question asks you to design convolutional neural networks (CNNs). Only the number of convolutional (Conv) layers and the number of fully connected (FC) layers will be specified below. You are free to design other aspects of the network. For example, you can use other types of operation (e.g. padding), layers (e.g. pooling, or preprocessing (e.g. augmentation), and you choose the number of units/neurons in each layer. Likewise, you may choose the number of epochs and many other settings according to your accessible computational power.\n",
    "\n",
    "**a.** Design a CNN with two Conv layers and two FC layers. Train the model on the training set, use the validation set to choose the best design among at least three different choices, and test the chosen model on the test set. Report the four metrics M1 to M4 **[2 marks]**\n",
    "\n",
    "**b.** Design a CNN with three Conv layers and three FC layers. Train the model on the training set, use the validation set to choose the best design among at least three different choices, and test the chosen model on the test set. Report the four metrics M1 to M4 **[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8f14ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea10d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d46aa886",
   "metadata": {},
   "source": [
    "### C3 Performance comparison (2 marks)\n",
    "\n",
    "**c.** Summarise each of the four metrics from the two models in B2 using one or more bar graphs. **[1 mark]**\n",
    "\n",
    "**d.** Describe at least two observations interesting to you. **[1 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2291eadb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13148dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c2ae0d9",
   "metadata": {},
   "source": [
    "## D. Unsupervised learning on Fashion-MNIST [8 marks]\n",
    "\n",
    "Fashion-MNIST is a dataset of Zalando's article images, with examples shown above. It consists of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes: 0=T-shirt/top; 1=Trouser; 2=Pullover; 3=Dress; 4=Coat; 5=Sandal; 6=Shirt; 7=Sneaker; 8=Bag; 9=Ankle boot.\n",
    "\n",
    "Choose any two out of the 10 classes and use only the test data for these two chosen classes to complete tasks in this section. It will be better to finish reading the remaining part of this section before choosing the two classes. Again, you may choose any two and there is no “correct” answer about which two to choose but some choices may make your studies below more interesting than others.\n",
    "\n",
    "Use the PyTorch API for Fashion-MNIST to load the training/test data of Fashion-MNIST. You may refer to similar procedures in Lab 7 for CIFAR-10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df958095",
   "metadata": {},
   "source": [
    "### D1. Dimensionality reduction and clustering [7 marks]\n",
    "\n",
    "**a.** Apply PCA to all images of these two chosen classes. Visualise the top 5 eigenvectors as images and display them in the order of descending corresponding values (the one corresponding to the largest eigenvalue first). **[1 marks]**\n",
    "\n",
    "**b.** Use the top 30 PCs to reconstruct 10 images, with 5 from each class (any 5 images are fine from each class). Show these 10 pairs of reconstructed and original images. **[1 marks]**\n",
    "    \n",
    "**c.** Visualise the two-dimensional PCA representations of all data points in a 2D plane (i.e. using the top two PCs). Use different colours/markers for the two classes for better visualisation (Hint: You need to use the class labels here for visualisation). **[1 marks]**\n",
    "\n",
    "**d.** Use spectral clustering to cluster all data points as represented by the top two PCs (clustering of two-dimensional vectors, where each vector has two values, PC1 and PC2). Visualise the two clusters with different colours/markers in 2D. **[2 marks]**\n",
    "\n",
    "**e.** Design a new autoencoder with five Conv2d layers and five ConvTranspose2d layers. You are free to choose the activation functions and settings such as stride and padding. Train this new autoencoder on all images of these two chosen classes for at least 20 epochs. Plot the loss against the epoch. **[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea727e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f440b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348d3681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99e1b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfa8dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "512550a0",
   "metadata": {},
   "source": [
    "### D2 Observation [1 marks]\n",
    "\n",
    "Describe at least two observations interesting to you from D1 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a26981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94dd567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fddfd7fa",
   "metadata": {},
   "source": [
    "## The END of Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67cbac0",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
